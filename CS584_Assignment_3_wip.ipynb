{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled113.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O7FCpOvTFMe"
      },
      "source": [
        "from sklearn import datasets\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset():\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "def train_test_split(X, y):\n",
        "    idx = np.arange(len(X))\n",
        "    train_size = int(len(X) * 2/3)\n",
        "    np.random.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WK-vqZgTUGD"
      },
      "source": [
        "def init_weights(num_in, num_out):\n",
        "  W = np.zeros((1 + num_in, num_out))\n",
        "  sd = np.sqrt(6.0 / (num_in + num_out))\n",
        "  for i in range(num_in):\n",
        "    for j in range(num_out):\n",
        "      x = np.float32(np.random.uniform(-sd, sd))\n",
        "      W[i,j] = x\n",
        "  return W\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8px2eITCTpEu"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3oH7d_bTweT"
      },
      "source": [
        "def sigmoid(x):\n",
        "  res = 1/ (1 + np.exp(-x))\n",
        "  return res\n",
        "\n",
        "def tanh(x):\n",
        "  res = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "  return res\n",
        "\n",
        "def sigmoid_gradient(x):\n",
        "  outcome = sigmoid(x)\n",
        "  grad = outcome * (1 - outcome)\n",
        "  return grad\n",
        "\n",
        "def tanh_gradient(x):\n",
        "  tanh_val = tanh(x)\n",
        "  grad = 1 - tanh_val **2\n",
        "  return grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txC_dieyIqF_",
        "outputId": "20d30e64-7fef-4db2-feef-c20524421aa7"
      },
      "source": [
        "y_train.size"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myHIjMV0bf3U"
      },
      "source": [
        "def forward(W, X):\n",
        "    '''\n",
        "    :param W: weights (including biases) of the neural network. It is a list containing both W_hidden and W_output.\n",
        "    :param X: input. Already added one additional column of all \"1\"s.\n",
        "    '''\n",
        "\n",
        "    # Shape of W_hidden: [num_feature+1, num_hidden]\n",
        "    # Shape pf W_output: [num_hidden+1, num_output]\n",
        "    W1 = np.reshape(initial_W[:50],(5,10))\n",
        "    W2 = np.reshape(initial_W[50:],(11,3))\n",
        "\n",
        "    z_hidden = X.dot(W1)\n",
        "    print(\"z1,\", z_hidden.shape)\n",
        "    a_hidden = tanh(z_hidden)\n",
        "    print(\"a1\",a_hidden.shape)\n",
        "\n",
        "    a_hidden = np.concatenate([np.ones((100, 1)), a_hidden], axis=1)\n",
        "    print(\"a1new\", a_hidden.shape)\n",
        "    z_output = a_hidden.dot(W2)\n",
        "    print(\"z2\",z_output.shape)\n",
        "\n",
        "    a_output = sigmoid(z_output)\n",
        "    print(\"a2\", a_output.shape)\n",
        "\n",
        "    #100*5 >> (5 * 10)() >> 10*3\n",
        "#ip >> (100*5)\n",
        "# w1 >> (5,10)\n",
        "#a1 >> (100*5)\n",
        "# z_h >> (100*10)\n",
        "# w2 >> (11,3)\n",
        "# a2 >> (100 * 11)\n",
        "# z_o >> (100*3)\n",
        "\n",
        "\n",
        "    return {'z_hidden': z_hidden, 'a_hidden': a_hidden,\n",
        "            'z_output': z_output, 'a_output': a_output}\n",
        "\n",
        "\n",
        "# Load data\n",
        "#X, y = load_dataset()\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Initialize weights\n",
        "#initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
        "#initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
        "#initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
        "\n",
        "#X_input = np.concatenate([np.ones((y_train.size, 1)), X_train], axis=1)\n",
        "#d = forward(initial_W, X_input)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGHkotsRUh7z"
      },
      "source": [
        "#W1 = np.reshape(initial_W[:50],(5,10)).T\n",
        "#X_input = np.concatenate([np.ones((y_train.size, 1)), X_train], axis=1)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYfOUzZDXL-N"
      },
      "source": [
        "def loss_funtion(W, X, y, num_feature, num_hidden, num_output, L2_lambda):\n",
        "    '''\n",
        "    :param W: a 1D array containing all weights and biases.\n",
        "    :param X: input\n",
        "    :param y: labels of X\n",
        "    :param num_feature: number of features in X\n",
        "    :param num_hidden: number of hidden units\n",
        "    :param num_output: number of output units\n",
        "    :param L2_lambda: the coefficient of regularization term\n",
        "    '''\n",
        "\n",
        "    m = y.size\n",
        "\n",
        "    # Reshape W back into the parameters W_hidden and W_output\n",
        "    W_hidden = np.reshape(W[:num_hidden * (num_feature + 1)],\n",
        "                          ((num_feature + 1), num_hidden))\n",
        "\n",
        "    W_output = np.reshape(W[(num_hidden * (num_feature + 1)):],\n",
        "                          ((num_hidden + 1), num_output))\n",
        "\n",
        "    # Initialize grads\n",
        "    W_hidden_grad = np.zeros(W_hidden.shape)\n",
        "    W_output_grad = np.zeros(W_output.shape)\n",
        "\n",
        "    # Add one column of \"1\"s to X\n",
        "    X_input = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    #one hot encoding y\n",
        "    num_classes = 3\n",
        "    y = np.squeeze(np.eye(num_classes)[y.reshape(-1)])\n",
        "    param_dict  = forward(W, X_input)\n",
        "    z_hidden = param_dict['z_hidden']\n",
        "    a_hidden = param_dict['a_hidden']\n",
        "    z_output = param_dict['z_output']\n",
        "    a_output = param_dict['a_output']\n",
        "\n",
        "\n",
        "    #cross entropy with l2 regularisatn\n",
        "    logprobs = np.multiply(y, np.log(a_output)) + np.multiply((1 - y), np.log(1 - a_output))\n",
        "    cross_entropy_cost = (-1.0/m) * np.sum(logprobs)\n",
        "\n",
        "    \n",
        "    L2_regularization_cost = (np.sum(np.square(W_hidden)) + np.sum(np.square(W_output)))*(L2_lambda/(2*m))\n",
        "\n",
        "    cost = cross_entropy_cost + L2_regularization_cost\n",
        "\n",
        "    #Compute the gradient for W_hidden and W_output (including both weights and biases) \n",
        "    W2 = W_output\n",
        "    W1 = W_hidden\n",
        "\n",
        "    dZ2 = a_output - y\n",
        "    dW2 = (1 / m) * np.dot(dZ2.T, a_hidden)\n",
        "    dZ1 = np.multiply(np.dot(W2, dZ2.T), 1 - np.power(a_hidden.T, 2))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X_input)\n",
        "\n",
        "    #grad_hidden = (1.0/m) * np.matmul(tanh_gradient(X), np.transpose(a_hidden)) + (L2_lambda/m)*W_hidden_grad\n",
        "\n",
        "    #grad_op = (1.0/m) * np.matmul(sigmoid_gradient(X), np.transpose(a_output)) + (L2_lambda/m)*W_output_grad\n",
        "\n",
        "\n",
        "    ##########################################################################################\n",
        "    # Full Mark: 3                                                                           #\n",
        "    # TODO:                                                                                  #\n",
        "    # 1. Transform y to one-hot encoding. Implement binary cross-entropy loss function       #\n",
        "    # (Hint: Use the forward function to get necessary outputs from the model)               #\n",
        "    #                                                                                        #\n",
        "    # 2. Add L2 regularization to all weights in loss                                        #\n",
        "    # (Note that we will not add regularization to bias)                                     #\n",
        "    #                                                                                        #\n",
        "    # 3. Compute the gradient for W_hidden and W_output (including both weights and biases)  #\n",
        "    # (Hint: use chain rule, and don't forget to add the gradient of regularization term)    #\n",
        "    ##########################################################################################\n",
        "    \n",
        "    L = cost\n",
        "    W_hidden_grad = dW1\n",
        "    W_output_grad = dW2\n",
        "    \n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
        "\n",
        "    return L, grads\n",
        "\n",
        "\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "cP38frDmX0OT",
        "outputId": "ba6c6cfb-5c2a-4025-804e-3df094c9e440"
      },
      "source": [
        "def optimize(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda):\n",
        "    '''\n",
        "    :param initial_W: initial weights as a 1D array.\n",
        "    :param X: input\n",
        "    :param y: labels of X\n",
        "    :param num_epoch: number of iterations\n",
        "    :param num_feature: number of features in X\n",
        "    :param num_hidden: number of hidden units\n",
        "    :param num_output: number of output units\n",
        "    :param L2_lambda: the coefficient of regularization term\n",
        "    '''\n",
        "\n",
        "    options = {'maxiter': num_epoch}\n",
        "    \n",
        "    ###########################################################################################\n",
        "    # Full Mark: 1                                                                            #\n",
        "    # TODO:                                                                                   #\n",
        "    # Optimize weights                                                                        #\n",
        "    # (Hint: use scipy.optimize.minimize to automatically do the iteration.)                  #\n",
        "    # ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) #\n",
        "    # For some optimizers, you need to set 'jac' as True.                                     #\n",
        "    # You may need to use lambda to create a function with one parameter to wrap the          #\n",
        "    # loss_funtion you have implemented above.                                                #\n",
        "    #                                                                                         #\n",
        "    # Note that scipy.optimize.minimize only accepts a 1D weight array as initial weights,    #\n",
        "    # and the output optimized weights will also be a 1D array.                               #\n",
        "    # That is why we unroll the initial weights into a single long vector.                    #\n",
        "    ###########################################################################################\n",
        "  l,grads = lambda e: loss_funtion(handler.handle, e)\n",
        "\n",
        "     = loss_funtion(initial_W, X, y,  num_feature, num_hidden, num_output, L2_lambda)\n",
        "\n",
        "\n",
        "\n",
        "    W_final =  minimize(,initial_W,method='BFGS')\n",
        "\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    # Obtain W_hidden and W_output back from W_final\n",
        "    W_hidden = np.reshape(W_final[:num_hidden * (num_feature + 1)],\n",
        "                          ((num_feature + 1), num_hidden))\n",
        "    W_output = np.reshape(W_final[(num_hidden * (num_feature + 1)):],\n",
        "                          ((num_hidden + 1), num_output))\n",
        "\n",
        "    return [W_hidden, W_output]\n",
        "\n",
        "\n",
        "# Define parameters\n",
        "NUM_FEATURE = 4\n",
        "NUM_HIDDEN_UNIT = 10\n",
        "NUM_OUTPUT_UNIT = 3\n",
        "NUM_EPOCH = 100\n",
        "L2_lambda = 1\n",
        "\n",
        "# Load data\n",
        "X, y = load_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Initialize weights\n",
        "initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
        "initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
        "initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
        "\n",
        "# Neural network learning\n",
        "W = optimize(initial_W, X_train, y_train, NUM_EPOCH, NUM_FEATURE, NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT, L2_lambda)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "z1, (100, 10)\n",
            "a1 (100, 10)\n",
            "a1new (100, 11)\n",
            "z2 (100, 3)\n",
            "a2 (100, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-6016598cabc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Neural network learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_FEATURE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_HIDDEN_UNIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_OUTPUT_UNIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-6016598cabc6>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m###########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mW_final\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_funtion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'BFGS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bfgs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_bfgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'newton-cg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0mfunc_calls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0mold_fval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfprime\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukqzdt_CSvBd"
      },
      "source": [
        "def predict(X_test, y_test, W):\n",
        "    '''\n",
        "    :param X_test: input\n",
        "    :param y_test: labels of X_test\n",
        "    :param W: a list containing two weights W_hidden and W_output.\n",
        "    '''\n",
        "\n",
        "    test_input = np.concatenate([np.ones((y_test.size, 1)), X_test], axis=1)\n",
        "    d = forward(W, X)\n",
        "    y_pred = d['a_output']\n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Predict on test set and compute the accuracy.                                   #\n",
        "    # (Hint: use forward function to get predicted output)                            #\n",
        "    #                                                                                 #\n",
        "    ###################################################################################\n",
        "\n",
        "    acc = \n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}